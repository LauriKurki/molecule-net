{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ase\n",
    "from ase.visualize import view\n",
    "\n",
    "import ml_collections\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from molnet.data import augmentation, input_pipeline_water\n",
    "from configs import root_dirs\n",
    "from configs.tests import water_test\n",
    "\n",
    "from typing import Any, Dict, List, Tuple, Sequence, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"water-bilayer-tf\"\n",
    "\n",
    "config = water_test.get_config()\n",
    "config.root_dir = root_dirs.get_root_dir(dataset)\n",
    "\n",
    "config.train_molecules = (0, 8000)\n",
    "config.val_molecules = (8000, 12000)\n",
    "\n",
    "config.target_z_cutoff = 1.5\n",
    "config.z_cutoff = 1.5\n",
    "config.interpolate_input_z = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = input_pipeline_water.get_datasets(config)['val']\n",
    "batch = next(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in batch.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_species = batch[\"atom_map\"].shape[-1]\n",
    "num_heights = batch[\"atom_map\"].shape[-2]\n",
    "index = 0\n",
    "\n",
    "fig = plt.figure(figsize=(num_heights*1.5, (1+num_species)*1.5))\n",
    "for i in range(num_heights):\n",
    "    ax = plt.subplot(num_species+1, num_heights, i + 1)\n",
    "    ax.imshow(batch[\"images\"][index, :, :, i, 0])\n",
    "\n",
    "    ax = plt.subplot(num_species+1, num_heights, num_heights + i + 1)\n",
    "    ax.imshow(batch[\"atom_map\"][index, :, :, i, 0])\n",
    "\n",
    "    ax = plt.subplot(num_species+1, num_heights, 2*num_heights + i + 1)\n",
    "    ax.imshow(batch[\"atom_map\"][index, :, :, i, 1])\n",
    "\n",
    "xyz = batch[\"xyz\"][index]\n",
    "xyz = xyz[xyz[:, -1] > 0]\n",
    "mol = ase.Atoms(\n",
    "    positions=xyz[:, :3],\n",
    "    numbers=xyz[:, -1].astype(int),\n",
    ")\n",
    "\n",
    "#view(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch[\"images\"]\n",
    "y = batch[\"atom_map\"]\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    fig = plt.figure(figsize=(16, 4))\n",
    "    ax = plt.subplot(141)\n",
    "    im = ax.imshow(x[i, ..., -5, 0], cmap='gray')\n",
    "    plt.colorbar(im)\n",
    "\n",
    "    ax = plt.subplot(142)\n",
    "    im = ax.imshow(y[i, ..., 0].mean(axis=-1), cmap='hot')\n",
    "    plt.colorbar(im)\n",
    "\n",
    "    ax = plt.subplot(143)\n",
    "    im = ax.imshow(y[i, ..., 1].mean(axis=-1), cmap='hot')\n",
    "    plt.colorbar(im)\n",
    "\n",
    "    ax = plt.subplot(144)\n",
    "    im = ax.imshow(y[i].mean(axis=-1).sum(axis=-1), cmap='hot')\n",
    "    plt.colorbar(im)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes to the pipeline can be tested here easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(\n",
    "    config: ml_collections.ConfigDict,\n",
    ") -> Dict[str, tf.data.Dataset]:\n",
    "    \"\"\"Loads datasets for each split.\"\"\"\n",
    "\n",
    "    filenames = sorted(os.listdir(config.root_dir))\n",
    "    filenames = [\n",
    "        os.path.join(config.root_dir, f)\n",
    "        for f in filenames\n",
    "        if f.startswith(\"afms_\")\n",
    "    ]\n",
    "\n",
    "    if len(filenames) == 0:\n",
    "        raise ValueError(f\"No files found in {config.root_dir}.\")\n",
    "    \n",
    "    # Partition the filenames into train, val, and test.\n",
    "    def filter_by_molecule_number(\n",
    "        filenames: Sequence[str], start: int, end: int\n",
    "    ) -> List[str]:\n",
    "        def filter_file(filename: str, start: int, end: int) -> bool:\n",
    "            filename = os.path.basename(filename)\n",
    "            file_start, file_end = [int(val) for val in re.findall(r\"\\d+\", filename)]\n",
    "            return start <= file_start and file_end <= end\n",
    "\n",
    "        return [f for f in filenames if filter_file(f, start, end)]\n",
    "\n",
    "    # Number of molecules for training can be smaller than the chunk size.\n",
    "    files_by_split = {\n",
    "        \"train\": filter_by_molecule_number(filenames, *config.train_molecules),\n",
    "        \"val\": filter_by_molecule_number(filenames, *config.val_molecules),\n",
    "    }\n",
    "\n",
    "    element_spec = tf.data.Dataset.load(filenames[0]).element_spec\n",
    "    datasets = {}\n",
    "    for split, files_split in files_by_split.items():\n",
    "\n",
    "        # Load the files.\n",
    "        dataset_split = tf.data.Dataset.from_tensor_slices(files_split)\n",
    "        # Shuffle the files.\n",
    "        dataset_split = dataset_split.shuffle(1000)\n",
    "        dataset_split = dataset_split.interleave(\n",
    "            lambda path: tf.data.Dataset.load(path, element_spec=element_spec),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "            deterministic=True,\n",
    "        )\n",
    "\n",
    "        # Repeat the dataset.\n",
    "        dataset_split = dataset_split.repeat()\n",
    "\n",
    "        # Shuffle the dataset.\n",
    "        if split == 'train':\n",
    "            dataset_split = dataset_split.shuffle(\n",
    "                1000,\n",
    "                reshuffle_each_iteration=True,\n",
    "            )\n",
    "\n",
    "        # batches consist of a dict {'x': image, 'xyz': xyz, 'sw': scan window})\n",
    "        dataset_split = dataset_split.map(\n",
    "            lambda x: {\n",
    "                \"images\": x[\"x\"],\n",
    "                \"xyz\": x[\"xyz\"],\n",
    "                \"sw\": x[\"sw\"],\n",
    "            },\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "            deterministic=True,\n",
    "        )\n",
    "\n",
    "        # Preprocess images.\n",
    "        dataset_split = dataset_split.map(\n",
    "            lambda x: _preprocess_batch(\n",
    "                x,\n",
    "                interpolate_z=config.interpolate_input_z,\n",
    "                z_cutoff=config.z_cutoff,\n",
    "            ),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "            deterministic=True,\n",
    "        )\n",
    "\n",
    "        dataset_split = dataset_split.map(\n",
    "            lambda x: _compute_atom_maps(\n",
    "                x,\n",
    "                z_cutoff=config.target_z_cutoff,\n",
    "                sigma=config.sigma,\n",
    "                factor=config.gaussian_factor,\n",
    "                dataset=config.dataset \n",
    "            ),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "            deterministic=True\n",
    "        )\n",
    "\n",
    "        # Crop images and atom maps.\n",
    "        dataset_split = dataset_split.map(\n",
    "            lambda x: _augment_image_and_atom_map(\n",
    "                x,\n",
    "                crop_size=128, # TODO: Make this a parameter in the config.\n",
    "                cutout_probs=config.cutout_probs,\n",
    "                noise_std=config.noise_std,\n",
    "                max_shift_per_slice=config.max_shift_per_slice,\n",
    "            ),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "            deterministic=True,\n",
    "        )\n",
    "\n",
    "        # Batch the dataset.\n",
    "        dataset_split = dataset_split.batch(config.batch_size)\n",
    "        dataset_split = dataset_split.prefetch(tf.data.AUTOTUNE).as_numpy_iterator()\n",
    "        \n",
    "        datasets[split] = dataset_split\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def _preprocess_batch(\n",
    "    batch: Dict[str, tf.Tensor],\n",
    "    interpolate_z: Optional[int] = None,\n",
    "    z_cutoff: float = 1.0,\n",
    ") -> Dict[str, tf.Tensor]:\n",
    "    \"\"\"Preprocesses images.\"\"\"\n",
    "    \n",
    "    x = batch[\"images\"]\n",
    "    x = tf.transpose(x, perm=[1, 0, 2])\n",
    "    x = x[::-1, :, :]\n",
    "\n",
    "    sw = batch[\"sw\"]\n",
    "\n",
    "    # Pad the xyz coordinates.\n",
    "    xyz = batch[\"xyz\"]\n",
    "    n_atoms = tf.shape(xyz)[0]\n",
    "    pad = 500 - n_atoms\n",
    "    xyz = tf.pad(xyz, [[0, pad], [0, 0]])\n",
    "\n",
    "    # Shift xyz coordinates by scan window, so that scan window starts at (0, 0).\n",
    "    #shifted_xyz = xyz[:, :2] - sw[0, :2]\n",
    "    #shifted_xyz = tf.concat([shifted_xyz, xyz[:, 2:]], axis=-1)\n",
    "\n",
    "    # Also shift the scan window to start at (0, 0).\n",
    "    #shifted_sw = sw - sw[0]\n",
    "\n",
    "    # Crop slices to z_cutoff.\n",
    "    z_slices = z_cutoff / 0.1\n",
    "    x = x[..., -int(z_slices):]\n",
    "\n",
    "\n",
    "    # Add channel dimension.\n",
    "    x = x[..., tf.newaxis]\n",
    "\n",
    "    # Interpolate to `interpolate_z` z slices\n",
    "    if interpolate_z is not None:\n",
    "        x = tf.image.resize(x, (x.shape[1], interpolate_z), method='bilinear')\n",
    "\n",
    "    sample = {\n",
    "        \"images\": x,\n",
    "        \"xyz\": xyz,\n",
    "        \"sw\": sw,\n",
    "    }\n",
    "    \n",
    "    return sample\n",
    "\n",
    "\n",
    "def _compute_atom_maps(\n",
    "    batch: Dict[str, tf.Tensor],\n",
    "    z_cutoff: float = 1.0,\n",
    "    sigma: float = 0.2,\n",
    "    factor: float = 5.0,\n",
    "    dataset: str = None,\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"Computes atom maps.\"\"\"\n",
    "    xyz = batch[\"xyz\"]\n",
    "    z_max = tf.reduce_max(xyz[:, 2])\n",
    "    sw = batch[\"sw\"]\n",
    "    xres = batch[\"images\"].shape[0]\n",
    "\n",
    "    # Compute grids # TODO: REPLACE WITH COORDINATES FROM BATCH[\"sw\"]\n",
    "    # Tried, didn't work. Come back to this later.\n",
    "    # For now, the molecule (and scan window) is shifted in _preprocess_images to start at (0, 0).\n",
    "    #x = tf.linspace(0., 32., 256)\n",
    "    #y = tf.linspace(0., 32., 256)\n",
    "    x = tf.linspace(sw[0,0], sw[1,0], xres)\n",
    "    y = tf.linspace(sw[0,1], sw[1,1], xres)\n",
    "    z_steps = tf.cast(z_cutoff / 0.1, tf.int32)\n",
    "    #z = tf.linspace(z_max-z_cutoff, z_max, z_steps)\n",
    "\n",
    "    # Reversing the z axis seems to be essential for smooth learning.\n",
    "    z = tf.linspace(z_max, z_max-z_cutoff, z_steps)\n",
    "\n",
    "    X, Y, Z = tf.meshgrid(x, y, z, indexing='xy')\n",
    "\n",
    "    # Compute atom maps.\n",
    "    maps_h = tf.zeros_like(X)\n",
    "    maps_o = tf.zeros_like(X)\n",
    "\n",
    "    for atom in xyz:\n",
    "        # Skip atoms that are not in the z_cutoff range.\n",
    "        if atom[2] < z_max - z_cutoff - 2*sigma:\n",
    "            continue\n",
    "\n",
    "        # Skip padding atoms\n",
    "        if atom[-1] == 0:\n",
    "            continue\n",
    "\n",
    "        m = tf.exp(\n",
    "            -((X - atom[0])**2 + (Y - atom[1])**2 + (Z - atom[2])**2) / (2 * sigma**2)\n",
    "        )\n",
    "        \n",
    "        # all values below 1e-4 to 0\n",
    "        m = tf.where(m < 1e-2, tf.zeros_like(m), m*factor)\n",
    "\n",
    "        if atom[-1] == 1:\n",
    "            maps_h += m\n",
    "        elif atom[-1] == 8:\n",
    "            maps_o += m\n",
    "\n",
    "    atom_map = tf.stack([maps_h, maps_o], axis=0)\n",
    "    atom_map = tf.transpose(atom_map, perm=[1, 2, 3, 0])\n",
    "\n",
    "    return {\n",
    "        \"images\": batch[\"images\"],\n",
    "        \"xyz\": batch[\"xyz\"],\n",
    "        \"sw\": batch[\"sw\"],\n",
    "        \"atom_map\": atom_map,\n",
    "    }\n",
    "\n",
    "def _augment_image_and_atom_map(\n",
    "    batch: Dict[str, tf.Tensor],\n",
    "    crop_size: int,\n",
    "    cutout_probs: List[float],\n",
    "    noise_std: float,\n",
    "    max_shift_per_slice: float,\n",
    ") -> Dict[str, tf.Tensor]:\n",
    "    \"\"\"Crops images and atom maps.\"\"\"\n",
    "    x = batch[\"images\"]\n",
    "    y = batch[\"atom_map\"]\n",
    "    xyz = batch[\"xyz\"]\n",
    "\n",
    "    # Normalize the images.\n",
    "    x = augmentation.normalize_images(x)\n",
    "\n",
    "    # Random rotation\n",
    "    x, y = augmentation.random_rotate_image_and_atom_map(x, y)\n",
    "\n",
    "    # Randomly shift the slices.\n",
    "    x = augmentation.random_slice_shift(x, max_shift_per_slice=max_shift_per_slice)\n",
    "\n",
    "    # Crop the images and atom maps.\n",
    "    #x, y = augmentation.random_crop_on_top_atom(x, y, xyz, shift=20, crop_size=crop_size)\n",
    "    x, y = augmentation.center_crop(x, y, size=crop_size, shift=10)\n",
    "\n",
    "    # Add noise to the images.\n",
    "    x = augmentation.add_noise(x, noise_std)\n",
    "\n",
    "    # Create cutouts.\n",
    "    x = augmentation.add_random_cutouts(\n",
    "        x, cutout_probs=cutout_probs, cutout_size_range=(2, 10), image_size=crop_size\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"images\": x,\n",
    "        \"atom_map\": y,\n",
    "        \"xyz\": batch[\"xyz\"],\n",
    "        \"sw\": batch[\"sw\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_datasets(config)\n",
    "batch = next(datasets['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first batch\n",
    "x = batch[\"images\"]\n",
    "y = batch[\"atom_map\"]\n",
    "\n",
    "num_images = x.shape[0]\n",
    "num_species = y.shape[-1]\n",
    "num_heights = y.shape[-2]\n",
    "\n",
    "for i in range(num_images):\n",
    "    fig = plt.figure(figsize=(16, 4))\n",
    "    ax = plt.subplot(141)\n",
    "    im = ax.imshow(x[i, ..., -5, 0], cmap='gray')\n",
    "    plt.colorbar(im)\n",
    "\n",
    "    for j in range(num_species):\n",
    "        ax = plt.subplot(1, num_species+2, j+2)\n",
    "        im = ax.imshow(y[i, ..., j].mean(axis=-1), cmap='hot')\n",
    "        plt.colorbar(im)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Molnet water pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molnet.data import input_pipeline_water\n",
    "from configs.tests import water_test\n",
    "from configs import root_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = water_test.get_config()\n",
    "config.root_dir = root_dirs.get_root_dir(config.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = input_pipeline_water.get_datasets(config)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch[\"images\"]\n",
    "y = batch[\"atom_map\"]\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    ax = plt.subplot(141)\n",
    "    ax.imshow(x[i, ..., 0].mean(axis=-1), cmap='gray')\n",
    "\n",
    "    ax = plt.subplot(142)\n",
    "    ax.imshow(y[i, ..., 0].mean(axis=-1), cmap='hot')\n",
    "\n",
    "    ax = plt.subplot(143)\n",
    "    ax.imshow(y[i, ..., 1].mean(axis=-1), cmap='hot')\n",
    "\n",
    "    ax = plt.subplot(144)\n",
    "    ax.imshow(y[i].mean(axis=(-1, -2)), cmap='hot')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-2.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
